{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "multi layer perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([0, 1, 1, 0])\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "np.random.seed(42)\n",
    "def multilayer_perceptron(X, y, epochs, alpha):\n",
    "    input_layer_neurons = 2\n",
    "    hidden_layer_neurons1 = 2\n",
    "    hidden_layer_neurons2 = 2\n",
    "    output_neurons = 1\n",
    "    W1 = np.random.uniform(size=(input_layer_neurons, hidden_layer_neurons1))\n",
    "    b1 = np.random.uniform(size=(1, hidden_layer_neurons1))\n",
    "    W2 = np.random.uniform(size=(hidden_layer_neurons1, hidden_layer_neurons2))\n",
    "    b2 = np.random.uniform(size=(1, hidden_layer_neurons2))\n",
    "    W3 = np.random.uniform(size=(hidden_layer_neurons2, output_neurons))\n",
    "    b3 = np.random.uniform(size=(1, output_neurons))\n",
    "    errors = []\n",
    "    for epoch in range(epochs):\n",
    "        hidden_layer_input1 = np.dot(X, W1) + b1\n",
    "        hidden_layer_output1 = sigmoid(hidden_layer_input1)\n",
    "        hidden_layer_input2 = np.dot(hidden_layer_output1, W2) + b2\n",
    "        hidden_layer_output2 = sigmoid(hidden_layer_input2)\n",
    "        output_layer_input = np.dot(hidden_layer_output2, W3) + b3\n",
    "        predicted_output = sigmoid(output_layer_input)\n",
    "        error = y.reshape(-1, 1) - predicted_output\n",
    "        errors.append(np.mean(np.square(error)))\n",
    "        d_predicted_output = error * sigmoid_derivative(predicted_output)\n",
    "        error_hidden_layer2 = d_predicted_output.dot(W3.T)\n",
    "        d_hidden_layer2 = error_hidden_layer2 * sigmoid_derivative(hidden_layer_output2)\n",
    "        error_hidden_layer1 = d_hidden_layer2.dot(W2.T)\n",
    "        d_hidden_layer1 = error_hidden_layer1 * sigmoid_derivative(hidden_layer_output1)\n",
    "        W3 += hidden_layer_output2.T.dot(d_predicted_output) * alpha\n",
    "        b3 += np.sum(d_predicted_output, axis=0, keepdims=True) * alpha\n",
    "        W2 += hidden_layer_output1.T.dot(d_hidden_layer2) * alpha\n",
    "        b2 += np.sum(d_hidden_layer2, axis=0, keepdims=True) * alpha\n",
    "        W1 += X.T.dot(d_hidden_layer1) * alpha\n",
    "        b1 += np.sum(d_hidden_layer1, axis=0, keepdims=True) * alpha\n",
    "    return W1, b1, W2, b2, W3, b3, errors\n",
    "\n",
    "def test(X, W1, b1, W2, b2,w3,b3):\n",
    "    hidden_layer_input1 = np.dot(X, W1) + b1\n",
    "    hidden_layer_output1 = sigmoid(hidden_layer_input1)\n",
    "    hidden_layer_input2 = np.dot(hidden_layer_output1, W2) + b2\n",
    "    hidden_layer_output2 = sigmoid(hidden_layer_input2)\n",
    "    output_layer_input = np.dot(hidden_layer_output2, W3) + b3\n",
    "    predicted_output = sigmoid(output_layer_input)\n",
    "    return np.round(predicted_output)\n",
    "alpha = 0.1\n",
    "epochs = 9000\n",
    "W1, b1, W2, b2, W3,b3,errors = multilayer_perceptron(X, y, epochs, alpha)\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, epochs + 1), errors)\n",
    "plt.title(\"Error vs. Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Mean Absolute Error\")\n",
    "predicted_output = test(X, W1, b1, W2, b2,W3,b3)\n",
    "print(\"Predicted Output:\\n\", predicted_output.flatten())\n",
    "print(\"Actual Output:\\n\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "single layer perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([0, 0, 0, 1])\n",
    "\n",
    "def hard_function(x):\n",
    "    return 1 if x >= 0 else 0\n",
    "\n",
    "def perceptron(epochs, alpha, bias):\n",
    "    W = np.random.rand(2)\n",
    "    errors = []\n",
    "    while epochs > 0:\n",
    "        total_error = 0\n",
    "        for i in range(4):\n",
    "            pred = hard_function(W.dot(X[i]) + bias)\n",
    "            error = y[i] - pred\n",
    "            total_error += abs(error)\n",
    "            W = W + (alpha * error) * X[i]\n",
    "            bias = bias + (alpha * error)\n",
    "        errors.append(total_error)\n",
    "        epochs -= 1\n",
    "    return W, bias, errors\n",
    "\n",
    "def test(x1, x2, weights, bias):\n",
    "    x1 = np.array(x1)\n",
    "    x2 = np.array(x2)\n",
    "    ans1s=[]\n",
    "    for i in range(4):\n",
    "        ans1 = (weights[0] * x1[i] + weights[1] * x2[i] + bias)\n",
    "        s=hard_function(ans1)\n",
    "        ans1s.append(s)\n",
    "    return ans1s\n",
    "\n",
    "\n",
    "alpha = 0.1\n",
    "epochs = 100\n",
    "bias = 0\n",
    "weights, bias, errors = perceptron(epochs, alpha, bias)\n",
    "X1 = [0, 0, 1, 1]\n",
    "X2 = [0, 1, 0, 1]\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(X1[0], X2[0], 'ro')\n",
    "plt.plot(X1[1], X2[1], 'ro')\n",
    "plt.plot(X1[2], X2[2], 'ro')\n",
    "plt.plot(X1[3], X2[3],'bo')\n",
    "pnt1 = (-bias / weights[0])\n",
    "pnt2 = (-bias / weights[1])\n",
    "point1 = [0, pnt1]\n",
    "point2 = [pnt2, 0]\n",
    "plt.plot(point1, point2)\n",
    "plt.title(\"Decision Boundary\")\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, epochs + 1), errors)\n",
    "plt.title(\"Error vs. Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Total Error\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "x1 = [0, 0, 1, 1]\n",
    "x2 = [0, 1, 0, 1]\n",
    "answer=(test(x1, x2, weights, bias))\n",
    "answer=np.array(answer)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(data_train, labels_train), (data_test, labels_test) = mnist.load_data()\n",
    "selected_nums = [0, 1]\n",
    "train_mask = np.isin(labels_train, selected_nums)\n",
    "test_mask = np.isin(labels_test, selected_nums)\n",
    "\n",
    "data_train, labels_train = data_train[train_mask], labels_train[train_mask]\n",
    "data_test, labels_test = data_test[test_mask], labels_test[test_mask]\n",
    "data_train = data_train / 255.0\n",
    "data_test = data_test / 255.0\n",
    "\n",
    "data_train = data_train.reshape(-1, 28, 28, 1)\n",
    "data_test = data_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "labels_train = keras.utils.to_categorical(labels_train, num_classes=2)\n",
    "labels_test = keras.utils.to_categorical(labels_test, num_classes=2)\n",
    "\n",
    "cnn_model = keras.Sequential([\n",
    "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "cnn_model.fit(data_train, labels_train, epochs=10, batch_size=32, validation_data=(data_test, labels_test))\n",
    "\n",
    "eval_loss, eval_acc = cnn_model.evaluate(data_test, labels_test)\n",
    "print(f\"Test Accuracy: {eval_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "xtrain=np.linspace(-1,1,100).reshape(-1, 1)\n",
    "ytrain=1+(np.sin(2*np.pi*xtrain)).reshape(-1, 1)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(50, activation='tanh', input_shape=(1,)),\n",
    "    Dense(32, activation='tanh'),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mean_squared_error\",optimizer=\"adam\", metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, Add, Input, GlobalAveragePooling2D, Flatten, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "(X_train, y_train),(X_test,y_test)=mnist.load_data()\n",
    "X_train=X_train.reshape(-1,28,28,1)/255.0\n",
    "X_test=X_test.reshape(-1,28,28,1)/255.0\n",
    "y_train=to_categorical(y_train,10)\n",
    "y_test=to_categorical(y_test,10)\n",
    "def bottleneck(x,filters,strides=1):\n",
    "  shortcut=x\n",
    "  x=Conv2D(filters,(1,1),strides=strides,padding='same')(x)\n",
    "  x=BatchNormalization()(x)\n",
    "  x=Activation('relu')(x)\n",
    "  x=Conv2D(filters,(3,3),padding='same')(x)\n",
    "  x=BatchNormalization()(x)\n",
    "  x=Activation('relu')(x)\n",
    "  x=Conv2D(filters*4,(1,1),padding='same')(x)\n",
    "  x=BatchNormalization()(x)\n",
    "  if strides != 1 or shortcut.shape[-1] != filters * 4:\n",
    "        shortcut = Conv2D(filters * 4, (1,1), strides=strides, padding='same')(shortcut)\n",
    "        shortcut = BatchNormalization()(shortcut)\n",
    "  x=Add()([x,shortcut])\n",
    "  x=Activation('relu')(x)\n",
    "  return x\n",
    "def resnet(layers,filters,inputshape,numclasses):\n",
    "  inputs=Input(inputshape)\n",
    "  x=Conv2D(64,(3,3),padding='same',activation='relu')(inputs)\n",
    "  for i,blocks in enumerate(layers):\n",
    "    for j in range(blocks):\n",
    "      strides=1 if j>0 else 2\n",
    "      x=bottleneck(x,filters[i],strides)\n",
    "  x=GlobalAveragePooling2D()(x)\n",
    "  x=Flatten()(x)\n",
    "  outputs=Dense(numclasses,activation='softmax')(x)\n",
    "  return Model(inputs,outputs)\n",
    "resnetmodel=resnet([3,4,6,3],filters=[64,128,256,512],inputshape=(28,28,1),numclasses=10)\n",
    "resnetmodel.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "resnetmodel.fit(X_train,y_train,epochs=10,batch_size=32)\n",
    "loss,acc=resnetmodel.evaluate(X_test,y_test)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#cnn form sratch\n",
    "\n",
    "def conv(X,kernel):\n",
    "  outputsize=X.shape[0]-kernel.shape[0]+1\n",
    "  output=np.zeros((outputsize,outputsize))\n",
    "  for i in range(outputsize):\n",
    "    for j in range(outputsize):\n",
    "      output[i,j]=np.sum(X[i:i+kernel.shape[0],j:j+kernel.shape[1]])\n",
    "  return output\n",
    "def maxpool(X,poolsize=2):\n",
    "  outputsize=X.shape[0]//poolsize\n",
    "  output=np.zeros((outputsize,outputsize))\n",
    "  for i in range(outputsize):\n",
    "    for j in range(outputsize):\n",
    "      output[i,j]=np.max(X[i*poolsize:(i+1)*poolsize,j*poolsize:(j+1)*poolsize])\n",
    "  return output\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "(X_train,y_train),(X_test,y_test)=mnist.load_data()\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0  # Normalize\n",
    "y_train, y_test = to_categorical(y_train, 10), to_categorical(y_test, 10)\n",
    "kernel=np.random.randn(3,3)\n",
    "sample_image=X_train[0]\n",
    "conv_output = conv(sample_image, kernel)\n",
    "pooled_output = maxpool(conv_output)\n",
    "print(\"Convolution Output Shape:\", conv_output.shape)\n",
    "print(\"Pooled Output Shape:\", pooled_output.shape)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
