{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "multi layer perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([0, 1, 1, 0])\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "np.random.seed(42)\n",
    "def multilayer_perceptron(X, y, epochs, alpha):\n",
    "    input_layer_neurons = 2\n",
    "    hidden_layer_neurons1 = 2\n",
    "    hidden_layer_neurons2 = 2\n",
    "    output_neurons = 1\n",
    "    W1 = np.random.uniform(size=(input_layer_neurons, hidden_layer_neurons1))\n",
    "    b1 = np.random.uniform(size=(1, hidden_layer_neurons1))\n",
    "    W2 = np.random.uniform(size=(hidden_layer_neurons1, hidden_layer_neurons2))\n",
    "    b2 = np.random.uniform(size=(1, hidden_layer_neurons2))\n",
    "    W3 = np.random.uniform(size=(hidden_layer_neurons2, output_neurons))\n",
    "    b3 = np.random.uniform(size=(1, output_neurons))\n",
    "    errors = []\n",
    "    for epoch in range(epochs):\n",
    "        hidden_layer_input1 = np.dot(X, W1) + b1\n",
    "        hidden_layer_output1 = sigmoid(hidden_layer_input1)\n",
    "        hidden_layer_input2 = np.dot(hidden_layer_output1, W2) + b2\n",
    "        hidden_layer_output2 = sigmoid(hidden_layer_input2)\n",
    "        output_layer_input = np.dot(hidden_layer_output2, W3) + b3\n",
    "        predicted_output = sigmoid(output_layer_input)\n",
    "\n",
    "        error = y.reshape(-1, 1) - predicted_output\n",
    "        errors.append(np.mean(np.square(error)))\n",
    "        d_predicted_output = error * sigmoid_derivative(predicted_output)\n",
    "        error_hidden_layer2 = d_predicted_output.dot(W3.T)\n",
    "        d_hidden_layer2 = error_hidden_layer2 * sigmoid_derivative(hidden_layer_output2)\n",
    "        error_hidden_layer1 = d_hidden_layer2.dot(W2.T)\n",
    "        d_hidden_layer1 = error_hidden_layer1 * sigmoid_derivative(hidden_layer_output1)\n",
    "        \n",
    "        W3 += hidden_layer_output2.T.dot(d_predicted_output) * alpha\n",
    "        b3 += np.sum(d_predicted_output, axis=0, keepdims=True) * alpha\n",
    "        W2 += hidden_layer_output1.T.dot(d_hidden_layer2) * alpha\n",
    "        b2 += np.sum(d_hidden_layer2, axis=0, keepdims=True) * alpha\n",
    "        W1 += X.T.dot(d_hidden_layer1) * alpha\n",
    "        b1 += np.sum(d_hidden_layer1, axis=0, keepdims=True) * alpha\n",
    "    return W1, b1, W2, b2, W3, b3, errors\n",
    "\n",
    "def test(X, W1, b1, W2, b2,w3,b3):\n",
    "    hidden_layer_input1 = np.dot(X, W1) + b1\n",
    "    hidden_layer_output1 = sigmoid(hidden_layer_input1)\n",
    "    hidden_layer_input2 = np.dot(hidden_layer_output1, W2) + b2\n",
    "    hidden_layer_output2 = sigmoid(hidden_layer_input2)\n",
    "    output_layer_input = np.dot(hidden_layer_output2, W3) + b3\n",
    "    predicted_output = sigmoid(output_layer_input)\n",
    "    return np.round(predicted_output)\n",
    "alpha = 0.1\n",
    "epochs = 9000\n",
    "W1, b1, W2, b2, W3,b3,errors = multilayer_perceptron(X, y, epochs, alpha)\n",
    "plt.plot(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "single layer perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([0, 0, 0, 1])\n",
    "\n",
    "def hard_function(x):\n",
    "    return 1 if x >= 0 else 0\n",
    "\n",
    "def perceptron(epochs, alpha, bias):\n",
    "    W = np.random.rand(2)\n",
    "    errors = []\n",
    "    while epochs > 0:\n",
    "        total_error = 0\n",
    "        for i in range(4):\n",
    "            pred = hard_function(W.dot(X[i]) + bias)\n",
    "            error = y[i] - pred\n",
    "            total_error += abs(error)\n",
    "            W = W + (alpha * error) * X[i]\n",
    "            bias = bias + (alpha * error)\n",
    "        errors.append(total_error)\n",
    "        epochs -= 1\n",
    "    return W, bias, errors\n",
    "\n",
    "def test(x1, x2, weights, bias):\n",
    "    x1 = np.array(x1)\n",
    "    x2 = np.array(x2)\n",
    "    ans1s=[]\n",
    "    for i in range(4):\n",
    "        ans1 = (weights[0] * x1[i] + weights[1] * x2[i] + bias)\n",
    "        s=hard_function(ans1)\n",
    "        ans1s.append(s)\n",
    "    return ans1s\n",
    "\n",
    "\n",
    "alpha = 0.1\n",
    "epochs = 100\n",
    "bias = 0\n",
    "weights, bias, errors = perceptron(epochs, alpha, bias)\n",
    "X1 = [0, 0, 1, 1]\n",
    "X2 = [0, 1, 0, 1]\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(X1[0], X2[0], 'ro')\n",
    "plt.plot(X1[1], X2[1], 'ro')\n",
    "plt.plot(X1[2], X2[2], 'ro')\n",
    "plt.plot(X1[3], X2[3],'bo')\n",
    "pnt1 = (-bias / weights[0])\n",
    "pnt2 = (-bias / weights[1])\n",
    "point1 = [0, pnt1]\n",
    "point2 = [pnt2, 0]\n",
    "plt.plot(point1, point2)\n",
    "plt.title(\"Decision Boundary\")\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, epochs + 1), errors)\n",
    "plt.title(\"Error vs. Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Total Error\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "x1 = [0, 0, 1, 1]\n",
    "x2 = [0, 1, 0, 1]\n",
    "answer=(test(x1, x2, weights, bias))\n",
    "answer=np.array(answer)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(data_train, labels_train), (data_test, labels_test) = mnist.load_data()\n",
    "selected_nums = [0, 1]\n",
    "train_mask = np.isin(labels_train, selected_nums)\n",
    "test_mask = np.isin(labels_test, selected_nums)\n",
    "\n",
    "data_train, labels_train = data_train[train_mask], labels_train[train_mask]\n",
    "data_test, labels_test = data_test[test_mask], labels_test[test_mask]\n",
    "data_train = data_train / 255.0\n",
    "data_test = data_test / 255.0\n",
    "\n",
    "data_train = data_train.reshape(-1, 28, 28, 1)\n",
    "data_test = data_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "labels_train = keras.utils.to_categorical(labels_train, num_classes=2)\n",
    "labels_test = keras.utils.to_categorical(labels_test, num_classes=2)\n",
    "\n",
    "cnn_model = keras.Sequential([\n",
    "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "cnn_model.fit(data_train, labels_train, epochs=10, batch_size=32, validation_data=(data_test, labels_test))\n",
    "\n",
    "eval_loss, eval_acc = cnn_model.evaluate(data_test, labels_test)\n",
    "print(f\"Test Accuracy: {eval_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "xtrain=np.linspace(-1,1,100).reshape(-1, 1)\n",
    "ytrain=1+(np.sin(2*np.pi*xtrain)).reshape(-1, 1)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(50, activation='tanh', input_shape=(1,)),\n",
    "    Dense(32, activation='tanh'),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mean_squared_error\",optimizer=\"adam\", metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, Add, Input, GlobalAveragePooling2D, Flatten, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "(X_train, y_train),(X_test,y_test)=mnist.load_data()\n",
    "X_train=X_train.reshape(-1,28,28,1)/255.0\n",
    "X_test=X_test.reshape(-1,28,28,1)/255.0\n",
    "y_train=to_categorical(y_train,10)\n",
    "y_test=to_categorical(y_test,10)\n",
    "def bottleneck(x,filters,strides=1):\n",
    "  shortcut=x\n",
    "  x=Conv2D(filters,(1,1),strides=strides,padding='same')(x)\n",
    "  x=BatchNormalization()(x)\n",
    "  x=Activation('relu')(x)\n",
    "  x=Conv2D(filters,(3,3),padding='same')(x)\n",
    "  x=BatchNormalization()(x)\n",
    "  x=Activation('relu')(x)\n",
    "  x=Conv2D(filters*4,(1,1),padding='same')(x)\n",
    "  x=BatchNormalization()(x)\n",
    "  if strides != 1 or shortcut.shape[-1] != filters * 4:\n",
    "        shortcut = Conv2D(filters * 4, (1,1), strides=strides, padding='same')(shortcut)\n",
    "        shortcut = BatchNormalization()(shortcut)\n",
    "  x=Add()([x,shortcut])\n",
    "  x=Activation('relu')(x)\n",
    "  return x\n",
    "def resnet(layers,filters,inputshape,numclasses):\n",
    "  inputs=Input(inputshape)\n",
    "  x=Conv2D(64,(3,3),padding='same',activation='relu')(inputs)\n",
    "  for i,blocks in enumerate(layers):\n",
    "    for j in range(blocks):\n",
    "      strides=1 if j>0 else 2\n",
    "      x=bottleneck(x,filters[i],strides)\n",
    "  x=GlobalAveragePooling2D()(x)\n",
    "  x=Flatten()(x)\n",
    "  outputs=Dense(numclasses,activation='softmax')(x)\n",
    "  return Model(inputs,outputs)\n",
    "resnetmodel=resnet([3,4,6,3],filters=[64,128,256,512],inputshape=(28,28,1),numclasses=10)\n",
    "resnetmodel.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "resnetmodel.fit(X_train,y_train,epochs=10,batch_size=32)\n",
    "loss,acc=resnetmodel.evaluate(X_test,y_test)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#cnn form sratch\n",
    "\n",
    "def conv(X,kernel):\n",
    "  outputsize=X.shape[0]-kernel.shape[0]+1\n",
    "  output=np.zeros((outputsize,outputsize))\n",
    "  for i in range(outputsize):\n",
    "    for j in range(outputsize):\n",
    "      output[i,j]=np.sum(X[i:i+kernel.shape[0],j:j+kernel.shape[1]])\n",
    "  return output\n",
    "def maxpool(X,poolsize=2):\n",
    "  outputsize=X.shape[0]//poolsize\n",
    "  output=np.zeros((outputsize,outputsize))\n",
    "  for i in range(outputsize):\n",
    "    for j in range(outputsize):\n",
    "      output[i,j]=np.max(X[i*poolsize:(i+1)*poolsize,j*poolsize:(j+1)*poolsize])\n",
    "  return output\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "(X_train,y_train),(X_test,y_test)=mnist.load_data()\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0  # Normalize\n",
    "y_train, y_test = to_categorical(y_train, 10), to_categorical(y_test, 10)\n",
    "kernel=np.random.randn(3,3)\n",
    "sample_image=X_train[0]\n",
    "conv_output = conv(sample_image, kernel)\n",
    "pooled_output = maxpool(conv_output)\n",
    "print(\"Convolution Output Shape:\", conv_output.shape)\n",
    "print(\"Pooled Output Shape:\", pooled_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Concatenate, Input\n",
    "\n",
    "# U-Net Architecture\n",
    "def unet_model(input_size=(128, 128, 1)):\n",
    "    inputs = Input(input_size)\n",
    "    c1 = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    p1 = MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "    c2 = Conv2D(128, (3, 3), activation='relu', padding='same')(p1)\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "    c3 = Conv2D(256, (3, 3), activation='relu', padding='same')(p2)\n",
    "\n",
    "    u1 = UpSampling2D((2, 2))(c3)\n",
    "    m1 = Concatenate()([u1, c2])\n",
    "    c4 = Conv2D(128, (3, 3), activation='relu', padding='same')(m1)\n",
    "\n",
    "    u2 = UpSampling2D((2, 2))(c4)\n",
    "    m2 = Concatenate()([u2, c1])\n",
    "    c5 = Conv2D(64, (3, 3), activation='relu', padding='same')(m2)\n",
    "\n",
    "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c5)\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "# Compile model\n",
    "model = unet_model()\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cnn padding scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def pad_image(X, pad):\n",
    "    \"\"\"Pads the image with zeros.\"\"\"\n",
    "    return np.pad(X, ((pad, pad), (pad, pad)), mode='constant')\n",
    "\n",
    "def conv(X, kernel, stride=1, padding=True):\n",
    "    \"\"\"Performs 2D convolution with optional padding and stride.\"\"\"\n",
    "    kernel_size = kernel.shape[0]\n",
    "    pad = (kernel_size - 1) // 2 if padding else 0\n",
    "    X_padded = pad_image(X, pad) if padding else X\n",
    "    \n",
    "    output_size = (X_padded.shape[0] - kernel_size) // stride + 1\n",
    "    output = np.zeros((output_size, output_size))\n",
    "\n",
    "    for i in range(0, output_size):\n",
    "        for j in range(0, output_size):\n",
    "            region = X_padded[i * stride : i * stride + kernel_size, j * stride : j * stride + kernel_size]\n",
    "            output[i, j] = np.sum(region * kernel)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def relu(X):\n",
    "    \"\"\"Applies ReLU activation.\"\"\"\n",
    "    return np.maximum(0, X)\n",
    "\n",
    "def maxpool(X, pool_size=2, stride=2):\n",
    "    \"\"\"Performs max pooling with stride.\"\"\"\n",
    "    output_size = (X.shape[0] - pool_size) // stride + 1\n",
    "    output = np.zeros((output_size, output_size))\n",
    "\n",
    "    for i in range(output_size):\n",
    "        for j in range(output_size):\n",
    "            output[i, j] = np.max(X[i * stride : i * stride + pool_size, j * stride : j * stride + pool_size])\n",
    "    \n",
    "    return output\n",
    "\n",
    "def flatten(X):\n",
    "    \"\"\"Flattens the feature map into a 1D array.\"\"\"\n",
    "    return X.flatten()\n",
    "\n",
    "def fully_connected(X, weights, bias):\n",
    "    \"\"\"Computes output of a fully connected layer.\"\"\"\n",
    "    return np.dot(X, weights) + bias\n",
    "\n",
    "def softmax(X):\n",
    "    \"\"\"Applies softmax activation.\"\"\"\n",
    "    exp_X = np.exp(X - np.max(X))  # Numerical stability\n",
    "    return exp_X / np.sum(exp_X)\n",
    "\n",
    "# Load MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0  # Normalize\n",
    "y_train, y_test = to_categorical(y_train, 10), to_categorical(y_test, 10)\n",
    "\n",
    "# Define a random 3x3 kernel\n",
    "kernel = np.random.randn(3, 3)\n",
    "\n",
    "# Select a sample image\n",
    "sample_image = X_train[0]\n",
    "\n",
    "# Forward pass\n",
    "conv_output = conv(sample_image, kernel, stride=1, padding=True)  # Convolution with padding\n",
    "relu_output = relu(conv_output)  # Apply ReLU\n",
    "pooled_output = maxpool(relu_output, pool_size=2, stride=2)  # Max pooling\n",
    "flattened_output = flatten(pooled_output)  # Flatten\n",
    "\n",
    "# Fully connected layer\n",
    "fc_weights = np.random.randn(flattened_output.shape[0], 10)  # Random weights\n",
    "fc_bias = np.random.randn(10)  # Random bias\n",
    "fc_output = fully_connected(flattened_output, fc_weights, fc_bias)  # Dense layer\n",
    "final_output = softmax(fc_output)  # Softmax activation\n",
    "\n",
    "# Print outputs\n",
    "print(\"Convolution Output Shape:\", conv_output.shape)\n",
    "print(\"ReLU Output Shape:\", relu_output.shape)\n",
    "print(\"Pooled Output Shape:\", pooled_output.shape)\n",
    "print(\"Flattened Output Shape:\", flattened_output.shape)\n",
    "print(\"Final Output (Class Probabilities):\", final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cnn - normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, utils, datasets\n",
    "\n",
    "# Load and preprocess data (only digits 0 and 1)\n",
    "(data_train, labels_train), (data_test, labels_test) = datasets.mnist.load_data()\n",
    "filter_digits = lambda data, labels: (data[labels < 2] / 255.0).reshape(-1, 28, 28, 1), utils.to_categorical(labels[labels < 2], 2)\n",
    "\n",
    "data_train, labels_train = filter_digits(data_train, labels_train)\n",
    "data_test, labels_test = filter_digits(data_test, labels_test)\n",
    "\n",
    "# Build CNN model\n",
    "cnn_model = tf.keras.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile, train, and evaluate\n",
    "cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "cnn_model.fit(data_train, labels_train, epochs=10, batch_size=32, validation_data=(data_test, labels_test))\n",
    "eval_loss, eval_acc = cnn_model.evaluate(data_test, labels_test)\n",
    "print(f\"Test Accuracy: {eval_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ann-normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, utils, datasets\n",
    "\n",
    "# Load and preprocess data (only digits 0 and 1)\n",
    "(data_train, labels_train), (data_test, labels_test) = datasets.mnist.load_data()\n",
    "filter_digits = lambda data, labels: (data[labels < 2] / 255.0).reshape(-1, 28 * 28), utils.to_categorical(labels[labels < 2], 2)\n",
    "\n",
    "data_train, labels_train = filter_digits(data_train, labels_train)\n",
    "data_test, labels_test = filter_digits(data_test, labels_test)\n",
    "\n",
    "# Build ANN model\n",
    "ann_model = tf.keras.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(28 * 28,)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile, train, and evaluate\n",
    "ann_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "ann_model.fit(data_train, labels_train, epochs=10, batch_size=32, validation_data=(data_test, labels_test))\n",
    "eval_loss, eval_acc = ann_model.evaluate(data_test, labels_test)\n",
    "print(f\"Test Accuracy: {eval_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "resnet normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, utils, datasets, applications\n",
    "\n",
    "# Load and preprocess data (only digits 0 and 1)\n",
    "(data_train, labels_train), (data_test, labels_test) = datasets.mnist.load_data()\n",
    "filter_digits = lambda data, labels: (tf.image.grayscale_to_rgb(tf.expand_dims(data[labels < 2] / 255.0, -1))\n",
    "                                      , utils.to_categorical(labels[labels < 2], 2))\n",
    "\n",
    "data_train, labels_train = filter_digits(data_train, labels_train)\n",
    "data_test, labels_test = filter_digits(data_test, labels_test)\n",
    "\n",
    "# Resize images to (224, 224, 3) for ResNet compatibility\n",
    "resize = lambda data: tf.image.resize(data, [224, 224])\n",
    "data_train, data_test = resize(data_train), resize(data_test)\n",
    "\n",
    "# Build ResNet50 model (without top, custom classifier)\n",
    "resnet_model = tf.keras.Sequential([\n",
    "    applications.ResNet50(weights=None, include_top=False, input_shape=(224, 224, 3)),\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile, train, and evaluate\n",
    "resnet_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "resnet_model.fit(data_train, labels_train, epochs=5, batch_size=32, validation_data=(data_test, labels_test))\n",
    "eval_loss, eval_acc = resnet_model.evaluate(data_test, labels_test)\n",
    "print(f\"Test Accuracy: {eval_acc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
